{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".ipynb settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Settings relative to .ipynb file format that have to be executed here\n",
    "\n",
    "#Make reloading of modules automatic et évite d'avoir à redémarer le kernel et tout re executer pour appliquer la modification d'un module dans __main__.\n",
    "%load_ext autoreload  \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src.useful_functions\n",
    "import sys\n",
    "sys.path.append('../src')  # Adjust the path to import modules in src/\n",
    "\n",
    "import useful_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of the dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The processed/X_train_info dataframe contains matchs of the seasons:  [2015 2016 2017 2018 2019 2020]\n",
      "The processed/X_test_info dataframe contains matchs of the seasons:  [2021 2022 2023]\n"
     ]
    }
   ],
   "source": [
    "#Importing dataset(s)\n",
    "from data import make_dataset\n",
    "\n",
    "#Load the feat_engineered_ds:\n",
    "X_train_info = make_dataset.load_data(True, 'processed/X_train_info')\n",
    "X_train_00   = make_dataset.load_data(False, 'processed/X_train_00')\n",
    "Y_train_00   = make_dataset.load_data(False, 'processed/Y_train_00')\n",
    "X_test_info  = make_dataset.load_data(True, 'processed/X_test_info')\n",
    "X_test_00    = make_dataset.load_data(False, 'processed/X_test_00')\n",
    "Y_test_00    = make_dataset.load_data(False, 'processed/Y_test_00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI) <u> Pipeline development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF1493;\"> <strong> <font size=\"4\">1) Pipeline and GridSearchCV implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#OU\\n\\n#Filter features selection\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_classif\\nfilter_feat_selector = SelectKBest(f_classif, k= 10)\\n\\n# --------------------------------------------------------------\\n# Scaler\\n# --------------------------------------------------------------\\n\\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\\nscaler = StandardScaler()\\n\\n\\n# --------------------------------------------------------------\\n# Model\\n# --------------------------------------------------------------\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression(penalty = \\'l1\\', fit_intercept=True, random_state = 999, solver = \\'saga\\', max_iter= 3000, verbose = False, tol=1e-4)\\n\\n# --------------------------------------------------------------\\n# Pipeline\\n# --------------------------------------------------------------\\nfrom sklearn.pipeline import Pipeline\\npipeline = Pipeline(steps=[(\"scaler\",scaler), (\"features_selector\",filter_feat_selector), (\"model\", model)])\\n\\n\\n#parameters to optimize definition\\nimport numpy as np\\nparameters = {\\'model__penalty\\': [\\'l1\\', \\'l2\\', \\'elasticnet\\', \\'None\\'],\\n             \\'model__C\\': np.logspace(-3, 1, 21),\\n             \\'model__solver\\': [\\'saga\\', \\'sag\\', \\'liblinear\\', \\'newton-cg\\', \\'lbfgs\\'],\\n             \\'features_selector__k\\' : [5,8,10,11,12,14,15,16,17]\\n             }\\n\\n# --------------------------------------------------------------\\n# GridSearchCV\\n# --------------------------------------------------------------\\n# Create a GridSearchCV object\\nfrom sklearn.model_selection import GridSearchCV\\nfrom configuration import constant_variables\\n\\ngrid_search = GridSearchCV(pipeline, parameters, cv=constant_variables.CV, scoring = \\'neg_log_loss\\', verbose=0)\\n# Fit the GridSearchCV to the data\\ngrid_search.fit(X_train_00.copy(), Y_train_00.copy().values.ravel())\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------------------------------\n",
    "# Correlated features remover (not used)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from features import features_selection\n",
    "\n",
    "\"\"\"\n",
    "#Correlated features remover definition (it's a transformer)\n",
    "corr_features_selector = features_selection.correlated_features_removal_transformer(corr_threshold = 0.95)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Features selector\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "model_sfs = LogisticRegression(max_iter=10000, C = 0.3, random_state =33, penalty = 'l2')\n",
    "sequ_feat_selector = SFS(estimator = model_sfs, k_features = 'best', forward = True, verbose = 0, cv=constant_variables.CV, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "\"\"\"\n",
    "\n",
    "#OU\n",
    "\n",
    "#Filter features selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "filter_feat_selector = SelectKBest(f_classif, k= 10)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Scaler\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Model\n",
    "# --------------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty = 'l1', fit_intercept=True, random_state = 999, solver = 'saga', max_iter= 3000, verbose = False, tol=1e-4)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Pipeline\n",
    "# --------------------------------------------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[(\"scaler\",scaler), (\"features_selector\",filter_feat_selector), (\"model\", model)])\n",
    "\n",
    "\n",
    "#parameters to optimize definition\n",
    "import numpy as np\n",
    "parameters = {'model__penalty': ['l1', 'l2', 'elasticnet', 'None'],\n",
    "             'model__C': np.logspace(-3, 1, 21),\n",
    "             'model__solver': ['saga', 'sag', 'liblinear', 'newton-cg', 'lbfgs'],\n",
    "             'features_selector__k' : [5,8,10,11,12,14,15,16,17]\n",
    "             }\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# GridSearchCV\n",
    "# --------------------------------------------------------------\n",
    "# Create a GridSearchCV object\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from configuration import constant_variables\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=constant_variables.CV, scoring = 'neg_log_loss', verbose=0)\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_00.copy(), Y_train_00.copy().values.ravel())\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF1493;\"> <strong> <font size=\"4\">2) Display GridSearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#We print the results of GridSearchCV() execution\\nresults.GridSearchCV_results(grid_search, X_train_00.copy())\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#src.pipeline.results\n",
    "\n",
    "from pipeline import results\n",
    "\"\"\"\n",
    "#We print the results of GridSearchCV() execution\n",
    "results.GridSearchCV_results(grid_search, X_train_00.copy())\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF1493;\"> <strong> <font size=\"4\">3) Model, features, scaler selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the old pipeline:     pipeline_03_trained\n",
      "Successfully saved the new pipeline:       pipeline_03_trained\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#src.pipeline.model\n",
    "from pipeline import model\n",
    "from features.features_selection import restricted_datasets\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#On récupère la pipeline avec ses parametres optimaux \n",
    "chosen_pipeline = grid_search.best_estimator_\n",
    "print(chosen_pipeline)\n",
    "\n",
    "X_train_01 = X_train_00.copy()\n",
    "X_test_01 = X_test_00.copy()\n",
    "\n",
    "chosen_pipeline_trained = chosen_pipeline.fit(X_train_01.copy(), Y_train_00.copy().values.ravel())\n",
    "\"\"\"\n",
    "#On choisit une pipeline enregistrée dans pipeline.model et la selection de features qui va avec\n",
    "\n",
    "chosen_pipeline = model.pipeline_03\n",
    "chosen_features = model.features_selected_03\n",
    "\n",
    "X_train_01 = X_train_00.copy()[chosen_features]\n",
    "\n",
    "X_test_01 = X_test_00.copy()[chosen_features]\n",
    "\n",
    "# Train the pipeline chosen\n",
    "chosen_pipeline_trained = chosen_pipeline.fit(X_train_01.copy(), Y_train_00.copy().values.ravel())\n",
    "\n",
    "#Save the trained pipeline\n",
    "\n",
    "model.save_pipeline(chosen_pipeline_trained, \"pipeline_03_trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#808080;\"> <strong> <font size=\"4\">4) Model calibration (Not used anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.calibration import CalibratedClassifierCV\\ncalibrated_pipeline = CalibratedClassifierCV(chosen_pipeline_trained, cv = 'prefit' , method = 'isotonic', ensemble = True)\\ncalibrated_pipeline.fit(X_valid_00.copy(), Y_valid_00.copy())\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#definition of a calibrator\n",
    "\"\"\"\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "calibrated_pipeline = CalibratedClassifierCV(chosen_pipeline_trained, cv = 'prefit' , method = 'isotonic', ensemble = True)\n",
    "calibrated_pipeline.fit(X_valid_00.copy(), Y_valid_00.copy())\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-projet-mbappe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
