projet_Mbappe
==============================

This project is a personal initiative that I am particularly proud of. Inspired by my professional aspirations in data science and my genuine interest in football and statistics, I decided to take on this instructive challenge. The project involves building a complex and comprehensive machine learning model to predict the probabilities of victory in football matches and identify value bets. It encompasses the entire process, from data extraction to betting strategy building.
 
Project Description
-------------------

- What my application does?

This project focuses on identifying value bets offered by bookmakers on football matches. A value bet occurs when bookmakers propose odds that underestimate a team's probability of winning, resulting in odds that are higher than they should be based on the actual probability

The program inputs teams stats since the beginning of the season. And outputs victories probabilities and Boolean value of whether the bet proposed by bookmaker is a value bet.

Where this bookmakers' flaw comes from?

This discrepancy between bookmakers' probabilities and the actual probabilities often arises because bookmakers factor in bettors' behaviors when calculating their odds. They do this to encourage balanced betting on both sides, minimizing their risk and ensuring a consistent profit margin regardless of the match outcome.
 

- How does it do it?

To do so the program tries to predict football matches outcomes (Win / Lose only for now)  probabilities and compare it to bookmakers one. The proba of bookmakers are the inverse of their odds.

The model chosen to do predictions is Logistic Regression. 


- Why I used the technologies I used?

Logistic Regression prooved to be the best model as it’s a model that computes probabilities to do classification. And the output needed for our model is probabilities. Moreover, the scoring function is directly based on the the proba predicted. Indeed, the more the predicted proba is far from the real outcome, the more the scoring function penalizes.


 


 

How to install and run the project
----------------------------------
I have not solid knowledge on code’s surrounding tools and executors, so I do not know it there are specific software or others required for my project…

I only use VS Code and the modules/packages listed in my ‘requirements’ file. However, I plan to set authentication or protection on my project.




Project Organization
------------


    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   ├── raw            <- The original, immutable data dump.
    │   └── results        <- Results and outputs generated by the pipeline.
    │
    ├── docs               <- Projects relative documentation
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │   ├── chosen_pipeline.pkl
    │   ├── chosen_pipeline_trained.pkl
    │   └── .gitkeep       
    │
    ├── notebooks          <- Jupyter notebooks
    │   ├── data_preparation.ipynb
    │   ├── features_exploration.ipynb
    │   ├── pipeline_dev.ipynb
    │   ├── pipeline_test.ipynb
    │   ├── results_final.ipynb
    │   ├── support_analysis.ipynb
    │   ├── user_test.ipynb
    │   ├── archive        <- Archived notebooks for backup and reference.
    │   │   ├── data_archive_code.ipynb
    │   │   └── X)_Model_exec_curr_seas.ipynb
    │   │   └── __main__.ipynb
    │   └── dvclive 
    │
    ├── references  
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │       ├── all_features_density_estimate.png
    │       ├── all_features_histo.png
    │       └── .gitkeep
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    ├── environment and requirements
    │   ├── environment_projet_mbappe.yml
    │   ├── requirements_proj_mbappe.txt
    │   ├── test_environment.py
    │   └── tox.ini
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── configuration  <- Configuration files and variables
    │   │   ├── constant_variables.py
    │   │   └── settings.py
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   ├── data_combination.py
        │   │   ├── make_dataset.py
    │   │   └── preprocessing.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   ├── features_selection.py
    │   │   ├── initialize_new_features_columns.py
    │   │   └── make_new_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   ├── pipeline       <- Scripts to handle analysis and pipeline results
    │   │   ├── analysis.py
    │   │   ├── model.py
    │   │   └── results.py
    │   │
    │   ├── visualization  <- Scripts to create exploratory and results oriented visualizations
    │   │   ├── visualize.py
    │   │   └── learning_curves.py
    │   │
    │   ├── tests.py       <- Unit tests and testing utilities
    │   ├── useful_functions.py <- Utility functions for the project
    │   └── user_test.py   <- Scripts for user testing and validation
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io



--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
